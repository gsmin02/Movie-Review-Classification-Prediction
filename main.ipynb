{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "559c8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de99e4",
   "metadata": {},
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea151daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ì„ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n",
    "from collections import Counter  # ì¹´ìš´í„° ìë£Œêµ¬ì¡°\n",
    "import os  # ìš´ì˜ì²´ì œ ì¸í„°í˜ì´ìŠ¤\n",
    "import platform  # í”Œë«í¼ ì •ë³´\n",
    "import re  # ì •ê·œ í‘œí˜„ì‹\n",
    "import sys  # ì‹œìŠ¤í…œ ì •ë³´\n",
    "import warnings  # ê²½ê³  ë©”ì‹œì§€ ì œì–´\n",
    "\n",
    "import matplotlib.pyplot as plt  # ë°ì´í„° ì‹œê°í™”\n",
    "plt.rc(\"font\", family=\"NanumBarunGothic\")  # í•œê¸€ í°íŠ¸ ì„¤ì •(ì—†ìœ¼ë©´ ì„¤ì¹˜ í•„ìš”)\n",
    "\n",
    "import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°\n",
    "import pandas as pd  # ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„\n",
    "import seaborn as sns  # ê³ ê¸‰ ì‹œê°í™”\n",
    "import torch  # ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
    "import koreanize_matplotlib\n",
    "import optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import accuracy_score, f1_score  # í‰ê°€ ì§€í‘œ\n",
    "from sklearn.model_selection import train_test_split  # ë°ì´í„° ë¶„í• \n",
    "\n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ ë° BERT ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,  # ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸\n",
    "    AutoTokenizer,  # í† í¬ë‚˜ì´ì €\n",
    "    DataCollatorWithPadding,  # íŒ¨ë”© ë°ì´í„° ì½œë ˆì´í„°\n",
    "    set_seed,\n",
    "    Trainer,  # íŠ¸ë ˆì´ë„ˆ\n",
    "    TrainingArguments,  # í›ˆë ¨ ì„¤ì •\n",
    ")\n",
    "\n",
    "# PyTorch ë°ì´í„° ì²˜ë¦¬\n",
    "from torch.utils.data import Dataset  # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë”\n",
    "# K-Fold êµì°¨ ê²€ì¦\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8bdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd131e",
   "metadata": {},
   "source": [
    "# GPU ì‚¬ìš©ì—¬ë¶€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c390b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PyTorch GPU ì§€ì› ì •ë³´ ===\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "CUDA ë²„ì „: 12.4\n",
      "GPU ê°œìˆ˜: 1\n",
      "í˜„ì¬ GPU: 0\n",
      "GPU ì´ë¦„: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(\"\\n=== PyTorch GPU ì§€ì› ì •ë³´ ===\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    print(f\"í˜„ì¬ GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU ì´ë¦„: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"CPUì—ì„œ ì‹¤í–‰ ì¤‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558ebec",
   "metadata": {},
   "source": [
    "# ëœë¤ ì‹œë“œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f21e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëœë¤ ì‹œë“œ 42ë¡œ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ëœë¤ ì‹œë“œ ì„¤ì •\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"ëœë¤ ì‹œë“œ {RANDOM_STATE}ë¡œ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6e2f5",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "359ed56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f8be9",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fbda2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ êµ¬ì„±\n",
    "class TextPreprocessingPipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_fitted = False\n",
    "        self.vocab_info = {}\n",
    "        self.label_patterns = {}\n",
    "\n",
    "    def basic_preprocess(self, texts):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "            cleaned = self._clean_text(text)\n",
    "            # ì •ê·œí™”\n",
    "            normalized = self._normalize_text(cleaned)\n",
    "            processed_texts.append(normalized)\n",
    "        return processed_texts\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = str(text).strip()\n",
    "        text = re.sub(r\"[ã„±-ã…Šã…Œã…ã…-ã…›ã…¡-ã…£]+\", \"\", text)\n",
    "        text = re.sub(r\"([ã…‹ã…])\\1{2,}\", r\"\\1\\1\", text)\n",
    "        text = re.sub(r\"([ã… ã…œã…¡])\\1{2,}\", r\"\\1\\1\", text)\n",
    "        text = re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\\1\", text)\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£.,!?ã…‹ã…ã… ã…œã…¡~\\-]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def _normalize_text(self, text):\n",
    "        # ì†Œë¬¸ì ë³€í™˜\n",
    "        text = text.lower()\n",
    "\n",
    "        # êµ¬ë‘ì  ì •ê·œí™”\n",
    "        text = re.sub(r\"[.]{2,}\", \".\", text)\n",
    "        text = re.sub(r\"[!]{2,}\", \"!\", text)\n",
    "        text = re.sub(r\"[?]{2,}\", \"?\", text)\n",
    "        text = re.sub(r\"[,]{2,}\", \",\", text)\n",
    "        text = re.sub(r\"\\s+([.,!?])\", r\"\\1\", text)\n",
    "        text = re.sub(r\"([.,!?])\\s+\", r\"\\1 \", text)\n",
    "\n",
    "        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "        text = re.sub(\n",
    "            r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "        text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def fit(self, texts, labels=None):\n",
    "        self.is_fitted = True\n",
    "        # print(\"âœ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
    "\n",
    "    def transform(self, texts):\n",
    "        if not self.is_fitted:\n",
    "            print(\"Warning: íŒŒì´í”„ë¼ì¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ì ìš©í•©ë‹ˆë‹¤.\")\n",
    "        return self.basic_preprocess(texts)\n",
    "\n",
    "    def fit_transform(self, texts, labels=None):\n",
    "        self.fit(texts, labels)\n",
    "        return self.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b72a72",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b01c8b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ êµ¬ì„± ì™„ë£Œ\n",
      "í˜„ì¬: ê¸°ë³¸ ì „ì²˜ë¦¬ ê¸°ëŠ¥ êµ¬í˜„ë¨\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "preprocessor = TextPreprocessingPipeline()\n",
    "print(\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ êµ¬ì„± ì™„ë£Œ\")\n",
    "print(\"í˜„ì¬: ê¸°ë³¸ ì „ì²˜ë¦¬ ê¸°ëŠ¥ êµ¬í˜„ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b8337",
   "metadata": {},
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4531e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (í† í¬ë‚˜ì´ì € ë³‘ë ¬ì²˜ë¦¬ ê²½ê³  í•´ê²°)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# GPU ì„¤ì •\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì„¤ì •\n",
    "NUM_CLASSES = 4  # í´ë˜ìŠ¤ ê°œìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c90ca",
   "metadata": {},
   "source": [
    "# GPU ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f31501d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU 1ê°œ ì‚¬ìš© ê°€ëŠ¥: cuda\n",
      "   GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"âœ… GPU {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥: {device}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í›ˆë ¨ ì§„í–‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bd309",
   "metadata": {},
   "source": [
    "# [Class] ReviewDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6e580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts, self.labels, self.tokenizer, self.max_length = (\n",
    "            texts,\n",
    "            labels,\n",
    "            tokenizer,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ë° íŒ¨ë”©\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,  # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¥´ê¸°\n",
    "            padding=\"max_length\",  # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",  # PyTorch í…ì„œë¡œ ë°˜í™˜\n",
    "        )\n",
    "\n",
    "        # ê¸°ë³¸ ì•„ì´í…œ êµ¬ì„± (input_ids, attention_mask)\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "        # labelsê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€ (train/validìš©)\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d96c65",
   "metadata": {},
   "source": [
    "# [Function] compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbf7063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),  # í´ë˜ìŠ¤ë³„ ê°€ì¤‘í‰ê·  F1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a29fb5",
   "metadata": {},
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9151c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì„¤ì •\n",
    "CHOSEN_MAX_LENGTH = {\"../tapt-electra\": 128, \"../tapt-roberta\": 128, \"../tapt-bert\": 128}\n",
    "NUM_CLASSES = 4  # í´ë˜ìŠ¤ ê°œìˆ˜ (0: ê°•í•œë¶€ì •, 1: ì•½í•œë¶€ì •, 2: ì•½í•œê¸ì •, 3: ê°•í•œê¸ì •)\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "BATCH_SIZE_TRAIN = {\"../tapt-electra\": 128, \"../tapt-roberta\": 64, \"../tapt-bert\": 64}\n",
    "BATCH_SIZE_EVAL = {\"../tapt-electra\": 64, \"../tapt-roberta\": 64, \"../tapt-bert\": 64}\n",
    "\n",
    "# í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "LEARNING_RATE = {\"../tapt-electra\": 1.276836788492535e-05, \"../tapt-roberta\": 1.131155727098881e-05, \"../tapt-bert\": 1.422517849886189e-05}\n",
    "NUM_EPOCHS = {\"../tapt-electra\": 3, \"../tapt-roberta\": 3, \"../tapt-bert\": 4}\n",
    "WARMUP_STEPS = {\"../tapt-electra\": 682, \"../tapt-roberta\": 668, \"../tapt-bert\": 696}\n",
    "WEIGHT_DECAY = {\"../tapt-electra\": 0.04553144387239281, \"../tapt-roberta\": 0.0980186282772229, \"../tapt-bert\": 0.002246181281329394}\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "SAVE_MODEL = False\n",
    "# wandb ì‚¬ìš© ì—¬ë¶€\n",
    "USE_WANDB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad6052",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "254e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„° ë¶„í• \n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train.tolist(), y_train.tolist())\n",
    "X_val_processed = preprocessor.transform(X_val.tolist())\n",
    "\n",
    "X_train_processed = pd.Series(X_train_processed)\n",
    "X_val_processed = pd.Series(X_val_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcb611",
   "metadata": {},
   "source": [
    "# ì•™ìƒë¸” ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2ed66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"../tapt-electra\",\n",
    "    \"../tapt-roberta\",\n",
    "    \"../tapt-bert\"\n",
    "]\n",
    "\n",
    "trainers = []\n",
    "tokenizers = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce641eb",
   "metadata": {},
   "source": [
    "# [Function] ëª¨ë¸ í•™ìŠµ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7821cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ë¡œ í›ˆë ¨ ê°„ì†Œí™”\n",
    "def train_model(model_name):\n",
    "    print(f\"ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES)\n",
    "\n",
    "    train_dataset = ReviewDataset(X_train_processed, y_train, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "    val_dataset = ReviewDataset(X_val_processed, y_val, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=NUM_EPOCHS[model_name],\n",
    "        per_device_train_batch_size=BATCH_SIZE_TRAIN[model_name],\n",
    "        per_device_eval_batch_size=BATCH_SIZE_EVAL[model_name],\n",
    "        warmup_steps=WARMUP_STEPS[model_name],\n",
    "        weight_decay=WEIGHT_DECAY[model_name],\n",
    "        learning_rate=LEARNING_RATE[model_name],\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\" if SAVE_MODEL else \"no\",\n",
    "        load_best_model_at_end=SAVE_MODEL,\n",
    "        metric_for_best_model=\"accuracy\" if SAVE_MODEL else None,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2 if SAVE_MODEL else 0,\n",
    "        report_to=\"wandb\" if USE_WANDB else \"none\",  # wandb ë¡œê¹… ì¡°ê±´ë¶€ í™œì„±í™”\n",
    "        run_name=f\"optuna_{model_name}\" if USE_WANDB else None,\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        logging_first_step=True,\n",
    "        save_safetensors=SAVE_MODEL,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73c348cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-electra\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5244' max='5244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5244/5244 19:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.420421</td>\n",
       "      <td>0.836707</td>\n",
       "      <td>0.834419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.393344</td>\n",
       "      <td>0.847935</td>\n",
       "      <td>0.845282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.394464</td>\n",
       "      <td>0.849759</td>\n",
       "      <td>0.847728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-roberta\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10488' max='10488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10488/10488 22:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.404400</td>\n",
       "      <td>0.406215</td>\n",
       "      <td>0.841552</td>\n",
       "      <td>0.838599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355700</td>\n",
       "      <td>0.375874</td>\n",
       "      <td>0.858162</td>\n",
       "      <td>0.855716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>0.385691</td>\n",
       "      <td>0.858073</td>\n",
       "      <td>0.857108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-bert\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13984' max='13984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13984/13984 30:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.409138</td>\n",
       "      <td>0.838924</td>\n",
       "      <td>0.835712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.379123</td>\n",
       "      <td>0.853764</td>\n",
       "      <td>0.851394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.405418</td>\n",
       "      <td>0.850009</td>\n",
       "      <td>0.849347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.418759</td>\n",
       "      <td>0.852333</td>\n",
       "      <td>0.851554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ê° ëª¨ë¸ í›ˆë ¨ ë£¨í”„\n",
    "for model_name in model_names:\n",
    "    trainer, tokenizer, model = train_model(model_name)\n",
    "    trainers.append(trainer)\n",
    "    tokenizers.append(tokenizer)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c29b2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "X = df['review']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e82f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ë¡œ í›ˆë ¨ ê°„ì†Œí™”\n",
    "def kfold_train_model(model_name):\n",
    "    print(f\"ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    X_processed = preprocessor.fit_transform(X.tolist(), y.tolist())\n",
    "    X_processed = pd.Series(X_processed)\n",
    "    \n",
    "    # StratifiedKFold(K=4) ì ìš©: í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ 4-fold CV ìˆ˜í–‰\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_STATE)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_processed, y_train)):\n",
    "        print(f\"ğŸ¤– Fold {fold+1}/4 ì‹œì‘\")\n",
    "        \n",
    "        # ê° foldì— ëŒ€í•œ ë°ì´í„° ë¶„í•  (X_train_processedì™€ y_trainì´ list ë˜ëŠ” arrayë¼ê³  ê°€ì •)\n",
    "        X_train_fold, X_val_fold = X_processed.iloc[train_idx], X_processed.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_dataset = ReviewDataset(X_train_fold, y_train_fold, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "        val_dataset = ReviewDataset(X_val_fold, y_val_fold, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "        \n",
    "        # ê° foldì— ëŒ€í•´ ìƒˆë¡œìš´ ëª¨ë¸ ë¡œë”©\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES)\n",
    "        \n",
    "        # TrainingArguments ì„¤ì • (foldë³„ output_dir ë³€ê²½í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results_fold_{fold}\",\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=BATCH_SIZE_TRAIN[model_name],\n",
    "            per_device_eval_batch_size=BATCH_SIZE_EVAL[model_name],\n",
    "            warmup_steps=WARMUP_STEPS[model_name],\n",
    "            weight_decay=WEIGHT_DECAY[model_name],\n",
    "            learning_rate=LEARNING_RATE[model_name],\n",
    "            logging_steps=100,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\" if SAVE_MODEL else \"no\",\n",
    "            load_best_model_at_end=SAVE_MODEL,\n",
    "            metric_for_best_model=\"accuracy\" if SAVE_MODEL else None,\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=2 if SAVE_MODEL else 0,\n",
    "            report_to=\"none\",\n",
    "            seed=RANDOM_STATE,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_num_workers=2,\n",
    "            remove_unused_columns=False,\n",
    "            push_to_hub=False,\n",
    "            gradient_accumulation_steps=1,\n",
    "            logging_first_step=True,\n",
    "            save_safetensors=SAVE_MODEL,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # fold í‰ê°€\n",
    "        eval_result = trainer.evaluate()\n",
    "        fold_metrics.append(eval_result)\n",
    "        print(f\"ğŸ¤– Fold {fold+1} í‰ê°€ ê²°ê³¼: {eval_result}\")\n",
    "    \n",
    "    # CV í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì¶œë ¥ (ì˜ˆ: accuracy)\n",
    "    if fold_metrics:\n",
    "        avg_accuracy = np.mean([m['eval_accuracy'] for m in fold_metrics if 'eval_accuracy' in m])\n",
    "        print(f\"ğŸ¤– 4-Fold CV í‰ê·  Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # ìµœì¢… ëª¨ë¸: ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ í›ˆë ¨ (ì›ë˜ val ì‚¬ìš©)\n",
    "    print(\"ğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_CLASSES)\n",
    "    \n",
    "    train_dataset = ReviewDataset(X_train_processed, y_train, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "    val_dataset = ReviewDataset(X_val_processed, y_val, tokenizer, CHOSEN_MAX_LENGTH[model_name])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=BATCH_SIZE_TRAIN[model_name],\n",
    "        per_device_eval_batch_size=BATCH_SIZE_EVAL[model_name],\n",
    "        warmup_steps=WARMUP_STEPS[model_name],\n",
    "        weight_decay=WEIGHT_DECAY[model_name],\n",
    "        learning_rate=LEARNING_RATE[model_name],\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\" if SAVE_MODEL else \"no\",\n",
    "        load_best_model_at_end=SAVE_MODEL,\n",
    "        metric_for_best_model=\"accuracy\" if SAVE_MODEL else None,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2 if SAVE_MODEL else 0,\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        logging_first_step=True,\n",
    "        save_safetensors=SAVE_MODEL,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9d88ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1/4 ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 05:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>0.437780</td>\n",
       "      <td>0.831182</td>\n",
       "      <td>0.827849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4377804696559906, 'eval_accuracy': 0.831181834435902, 'eval_f1': 0.8278488540794774, 'eval_runtime': 29.48, 'eval_samples_per_second': 1897.221, 'eval_steps_per_second': 29.647, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 2/4 ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 05:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.445300</td>\n",
       "      <td>0.445722</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.824968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 2 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4457216262817383, 'eval_accuracy': 0.8289468979080994, 'eval_f1': 0.8249678996963056, 'eval_runtime': 29.4633, 'eval_samples_per_second': 1898.295, 'eval_steps_per_second': 29.664, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 3/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 05:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.439436</td>\n",
       "      <td>0.830502</td>\n",
       "      <td>0.826849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 3 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4394364058971405, 'eval_accuracy': 0.8305024137314501, 'eval_f1': 0.8268494313643976, 'eval_runtime': 29.684, 'eval_samples_per_second': 1884.181, 'eval_steps_per_second': 29.443, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 4/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 05:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.439459</td>\n",
       "      <td>0.831379</td>\n",
       "      <td>0.828095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 4 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4394588768482208, 'eval_accuracy': 0.8313785088503487, 'eval_f1': 0.8280945951828784, 'eval_runtime': 29.5375, 'eval_samples_per_second': 1893.528, 'eval_steps_per_second': 29.59, 'epoch': 1.0}\n",
      "ğŸ¤– 4-Fold CV í‰ê·  Accuracy: 0.8305\n",
      "ğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1748' max='1748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1748/1748 06:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.835974</td>\n",
       "      <td>0.832250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1/4 ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.400484</td>\n",
       "      <td>0.843197</td>\n",
       "      <td>0.838991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4004836976528168, 'eval_accuracy': 0.8431968532093689, 'eval_f1': 0.8389905188183202, 'eval_runtime': 29.8369, 'eval_samples_per_second': 1874.527, 'eval_steps_per_second': 29.293, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 2/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.406368</td>\n",
       "      <td>0.842196</td>\n",
       "      <td>0.838035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 2 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.40636777877807617, 'eval_accuracy': 0.8421956016449132, 'eval_f1': 0.8380347328504419, 'eval_runtime': 29.7026, 'eval_samples_per_second': 1883.0, 'eval_steps_per_second': 29.425, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 3/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.399574</td>\n",
       "      <td>0.844198</td>\n",
       "      <td>0.840423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 3 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.3995736241340637, 'eval_accuracy': 0.8441981047738244, 'eval_f1': 0.8404234903189621, 'eval_runtime': 29.8039, 'eval_samples_per_second': 1876.599, 'eval_steps_per_second': 29.325, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 4/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>0.406111</td>\n",
       "      <td>0.841302</td>\n",
       "      <td>0.837417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 4 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4061111509799957, 'eval_accuracy': 0.8413016270337922, 'eval_f1': 0.8374170852868247, 'eval_runtime': 29.8378, 'eval_samples_per_second': 1874.467, 'eval_steps_per_second': 29.292, 'epoch': 1.0}\n",
      "ğŸ¤– 4-Fold CV í‰ê·  Accuracy: 0.8427\n",
      "ğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 07:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.388865</td>\n",
       "      <td>0.849401</td>\n",
       "      <td>0.845626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”© ë° í›ˆë ¨: ../tapt-bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1/4 ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.404141</td>\n",
       "      <td>0.841570</td>\n",
       "      <td>0.837244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 1 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4041411876678467, 'eval_accuracy': 0.8415698194171286, 'eval_f1': 0.8372439061678616, 'eval_runtime': 29.7056, 'eval_samples_per_second': 1882.812, 'eval_steps_per_second': 29.422, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 2/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.407985</td>\n",
       "      <td>0.839585</td>\n",
       "      <td>0.835482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 2 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.40798500180244446, 'eval_accuracy': 0.8395851957804399, 'eval_f1': 0.8354819918850078, 'eval_runtime': 29.634, 'eval_samples_per_second': 1887.36, 'eval_steps_per_second': 29.493, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 3/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.403618</td>\n",
       "      <td>0.841695</td>\n",
       "      <td>0.837568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 3 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4036180078983307, 'eval_accuracy': 0.8416949758626855, 'eval_f1': 0.8375676134675311, 'eval_runtime': 29.6339, 'eval_samples_per_second': 1887.365, 'eval_steps_per_second': 29.493, 'epoch': 1.0}\n",
      "ğŸ¤– Fold 4/4 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2622' max='2622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2622/2622 05:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.406110</td>\n",
       "      <td>0.841480</td>\n",
       "      <td>0.837408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Fold 4 í‰ê°€ ê²°ê³¼: {'eval_loss': 0.4061097800731659, 'eval_accuracy': 0.8414804219560165, 'eval_f1': 0.8374077273386935, 'eval_runtime': 29.6441, 'eval_samples_per_second': 1886.719, 'eval_steps_per_second': 29.483, 'epoch': 1.0}\n",
      "ğŸ¤– 4-Fold CV í‰ê·  Accuracy: 0.8411\n",
      "ğŸ¤– ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 07:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.391784</td>\n",
       "      <td>0.847559</td>\n",
       "      <td>0.843614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"../tapt-electra\",\n",
    "    \"../tapt-roberta\",\n",
    "    \"../tapt-bert\"\n",
    "]\n",
    "\n",
    "kfold_trainers = []\n",
    "kfold_tokenizers = []\n",
    "kfold_models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    trainer, tokenizer, model = kfold_train_model(model_name)\n",
    "    kfold_trainers.append(trainer)\n",
    "    kfold_tokenizers.append(tokenizer)\n",
    "    kfold_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe7b1a",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd55fa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ë° ì•™ìƒë¸” ì¶”ë¡ \n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "test_texts = df_test[\"review\"].tolist()\n",
    "test_processed = preprocessor.transform(test_texts)\n",
    "test_processed = pd.Series(test_processed)\n",
    "\n",
    "test_datasets = []\n",
    "# for i, tokenizer in enumerate(tokenizers):\n",
    "#     test_datasets.append(ReviewDataset(test_processed, None, tokenizer, CHOSEN_MAX_LENGTH))\n",
    "test_datasets.append(ReviewDataset(test_processed, None, tokenizers[0], CHOSEN_MAX_LENGTH[\"../tapt-electra\"]))\n",
    "test_datasets.append(ReviewDataset(test_processed, None, tokenizers[1], CHOSEN_MAX_LENGTH[\"../tapt-roberta\"]))\n",
    "test_datasets.append(ReviewDataset(test_processed, None, tokenizers[2], CHOSEN_MAX_LENGTH[\"../tapt-bert\"]))\n",
    "\n",
    "all_logits = []\n",
    "for i, trainer in enumerate(kfold_trainers): # trainers\n",
    "    pred = trainer.predict(test_datasets[i])\n",
    "    all_logits.append(pred.predictions)\n",
    "\n",
    "avg_logits = np.mean(all_logits, axis=0)\n",
    "predicted_labels = np.argmax(avg_logits, axis=1)\n",
    "\n",
    "df_test[\"pred\"] = predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce691a3c",
   "metadata": {},
   "source": [
    "# ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ab2dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:\n",
      "   ê°•í•œ ë¶€ì • (0): 25,975ê°œ (43.3%)\n",
      "   ì•½í•œ ë¶€ì • (1): 3,392ê°œ (5.7%)\n",
      "   ì•½í•œ ê¸ì • (2): 22,810ê°œ (38.1%)\n",
      "   ê°•í•œ ê¸ì • (3): 7,751ê°œ (12.9%)\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ë¶„ì„\n",
    "LABEL_MAPPING = {0: \"ê°•í•œ ë¶€ì •\", 1: \"ì•½í•œ ë¶€ì •\", 2: \"ì•½í•œ ê¸ì •\", 3: \"ê°•í•œ ê¸ì •\"}\n",
    "unique_predictions, counts = np.unique(predicted_labels, return_counts=True)\n",
    "print(\"\\ní´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "for pred, count in zip(unique_predictions, counts):\n",
    "    percentage = (count / len(predicted_labels)) * 100\n",
    "    class_name = LABEL_MAPPING.get(pred, f\"í´ë˜ìŠ¤ {pred}\")\n",
    "    print(f\"   {class_name} ({pred}): {count:,}ê°œ ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cce378",
   "metadata": {},
   "source": [
    "# ì œì¶œ íŒŒì¼ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03d67c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ í˜¸ì¶œ\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "submission_df = sample_submission[[\"ID\"]].merge(df_test[[\"ID\", \"pred\"]], on=\"ID\", how=\"left\")\n",
    "submission_df = submission_df[[\"ID\", \"pred\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06108f56",
   "metadata": {},
   "source": [
    "# ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c472e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ê²€ì¦ì´ í†µê³¼ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "assert len(submission_df) == len(sample_submission), (\n",
    "    f\"ê¸¸ì´ ë¶ˆì¼ì¹˜: submission_dfëŠ” {len(submission_df)}í–‰, sample_submissionì€ {len(sample_submission)}í–‰\"\n",
    ")\n",
    "assert submission_df[\"pred\"].isin([0, 1, 2, 3]).all(), (\n",
    "    \"ëª¨ë“  ì˜ˆì¸¡ê°’ì€ [0, 1, 2, 3] ë²”ìœ„ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
    ")\n",
    "assert not submission_df[\"pred\"].isnull().any(), \"ì˜ˆì¸¡ê°’ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
    "assert not submission_df[\"ID\"].isnull().any(), \"ID ì»¬ëŸ¼ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
    "print(\"âœ… ëª¨ë“  ê²€ì¦ì´ í†µê³¼ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da19afa",
   "metadata": {},
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5eaf24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: ./output14.csv\n"
     ]
    }
   ],
   "source": [
    "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "submission_path = \"./output14.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91cf9c4",
   "metadata": {},
   "source": [
    "# Oputna ì¶”ê°€ ì‘ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90873e01",
   "metadata": {},
   "source": [
    "## klue/roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a10ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 18:01:47,347] A new study created in memory with name: no-name-33dd570d-bdeb-40bd-bbb4-ffd637d19cb4\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10488' max='10488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10488/10488 22:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.434100</td>\n",
       "      <td>0.427240</td>\n",
       "      <td>0.834096</td>\n",
       "      <td>0.829281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.412934</td>\n",
       "      <td>0.842625</td>\n",
       "      <td>0.839122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.450862</td>\n",
       "      <td>0.844413</td>\n",
       "      <td>0.843758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 18:25:08,761] Trial 0 finished with value: 0.8437576509010117 and parameters: {'learning_rate': 3.508859601126384e-05, 'num_epochs': 3, 'batch_size_train': 64, 'batch_size_eval': 64, 'warmup_steps': 770, 'weight_decay': 0.006810423821776945, 'max_length': 64}. Best is trial 0 with value: 0.8437576509010117.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8740' max='8740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8740/8740 49:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.443808</td>\n",
       "      <td>0.830717</td>\n",
       "      <td>0.823127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.397936</td>\n",
       "      <td>0.845754</td>\n",
       "      <td>0.842418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.419452</td>\n",
       "      <td>0.848507</td>\n",
       "      <td>0.848884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.519656</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.849319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.609384</td>\n",
       "      <td>0.849455</td>\n",
       "      <td>0.849437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 19:15:57,931] Trial 1 finished with value: 0.8494372629723314 and parameters: {'learning_rate': 7.960057845425662e-05, 'num_epochs': 5, 'batch_size_train': 128, 'batch_size_eval': 256, 'warmup_steps': 533, 'weight_decay': 0.07100540218860062, 'max_length': 128}. Best is trial 1 with value: 0.8494372629723314.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 19:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.396245</td>\n",
       "      <td>0.845485</td>\n",
       "      <td>0.841769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.374091</td>\n",
       "      <td>0.856642</td>\n",
       "      <td>0.854702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 19:37:19,673] Trial 2 finished with value: 0.8547015839626576 and parameters: {'learning_rate': 1.8392945013901404e-05, 'num_epochs': 2, 'batch_size_train': 128, 'batch_size_eval': 128, 'warmup_steps': 554, 'weight_decay': 0.022905530992320077, 'max_length': 128}. Best is trial 2 with value: 0.8547015839626576.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 37:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.412377</td>\n",
       "      <td>0.841874</td>\n",
       "      <td>0.836808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.381536</td>\n",
       "      <td>0.853495</td>\n",
       "      <td>0.851070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.435928</td>\n",
       "      <td>0.851976</td>\n",
       "      <td>0.849949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.501849</td>\n",
       "      <td>0.852584</td>\n",
       "      <td>0.852510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 20:16:23,473] Trial 3 finished with value: 0.8525095707806003 and parameters: {'learning_rate': 9.162255026021497e-05, 'num_epochs': 4, 'batch_size_train': 256, 'batch_size_eval': 256, 'warmup_steps': 200, 'weight_decay': 0.0051416448004362285, 'max_length': 128}. Best is trial 2 with value: 0.8547015839626576.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10488' max='10488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10488/10488 32:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.402021</td>\n",
       "      <td>0.843501</td>\n",
       "      <td>0.839914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.374241</td>\n",
       "      <td>0.859360</td>\n",
       "      <td>0.856756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.399280</td>\n",
       "      <td>0.856732</td>\n",
       "      <td>0.855998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 20:50:47,462] Trial 4 finished with value: 0.8559977395510814 and parameters: {'learning_rate': 1.9317637883686442e-05, 'num_epochs': 3, 'batch_size_train': 64, 'batch_size_eval': 128, 'warmup_steps': 465, 'weight_decay': 0.009433977915424418, 'max_length': 128}. Best is trial 4 with value: 0.8559977395510814.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13984' max='13984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13984/13984 29:56, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.428932</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.829740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.416618</td>\n",
       "      <td>0.841927</td>\n",
       "      <td>0.838267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.438308</td>\n",
       "      <td>0.842374</td>\n",
       "      <td>0.841765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>0.842589</td>\n",
       "      <td>0.841887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 21:22:02,496] Trial 5 finished with value: 0.8418868323472474 and parameters: {'learning_rate': 2.0296858706164013e-05, 'num_epochs': 4, 'batch_size_train': 64, 'batch_size_eval': 128, 'warmup_steps': 328, 'weight_decay': 0.015510196080755715, 'max_length': 64}. Best is trial 4 with value: 0.8559977395510814.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1748' max='1748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1748/1748 12:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.437362</td>\n",
       "      <td>0.827642</td>\n",
       "      <td>0.824518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.413432</td>\n",
       "      <td>0.839353</td>\n",
       "      <td>0.836280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 21:35:28,876] Trial 6 finished with value: 0.8362801357528785 and parameters: {'learning_rate': 1.493229896328817e-05, 'num_epochs': 2, 'batch_size_train': 256, 'batch_size_eval': 64, 'warmup_steps': 298, 'weight_decay': 0.012357516050101863, 'max_length': 64}. Best is trial 4 with value: 0.8559977395510814.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4370' max='4370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4370/4370 30:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.458549</td>\n",
       "      <td>0.820257</td>\n",
       "      <td>0.817147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>0.413058</td>\n",
       "      <td>0.839013</td>\n",
       "      <td>0.836420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.419465</td>\n",
       "      <td>0.839156</td>\n",
       "      <td>0.835545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.415706</td>\n",
       "      <td>0.841910</td>\n",
       "      <td>0.839444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.424666</td>\n",
       "      <td>0.842571</td>\n",
       "      <td>0.840826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 22:06:55,958] Trial 7 finished with value: 0.8408262896353824 and parameters: {'learning_rate': 1.2798470733572483e-05, 'num_epochs': 5, 'batch_size_train': 256, 'batch_size_eval': 64, 'warmup_steps': 657, 'weight_decay': 0.017120969329243695, 'max_length': 64}. Best is trial 4 with value: 0.8559977395510814.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10488' max='10488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10488/10488 32:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.406502</td>\n",
       "      <td>0.839675</td>\n",
       "      <td>0.837014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.378276</td>\n",
       "      <td>0.856839</td>\n",
       "      <td>0.854554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.385231</td>\n",
       "      <td>0.857232</td>\n",
       "      <td>0.856226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 01:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 22:41:34,769] Trial 8 finished with value: 0.8562264320036017 and parameters: {'learning_rate': 1.131155727098881e-05, 'num_epochs': 3, 'batch_size_train': 64, 'batch_size_eval': 64, 'warmup_steps': 668, 'weight_decay': 0.0980186282772229, 'max_length': 128}. Best is trial 8 with value: 0.8562264320036017.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../tapt-roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4370' max='4370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4370/4370 29:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.455486</td>\n",
       "      <td>0.822439</td>\n",
       "      <td>0.818758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.416196</td>\n",
       "      <td>0.839245</td>\n",
       "      <td>0.836437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.422374</td>\n",
       "      <td>0.841230</td>\n",
       "      <td>0.837282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.427705</td>\n",
       "      <td>0.841802</td>\n",
       "      <td>0.839925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.454421</td>\n",
       "      <td>0.842124</td>\n",
       "      <td>0.841103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 23:12:47,407] Trial 9 finished with value: 0.8411026216920604 and parameters: {'learning_rate': 2.1804135496929557e-05, 'num_epochs': 5, 'batch_size_train': 256, 'batch_size_eval': 256, 'warmup_steps': 955, 'weight_decay': 0.06322696114512115, 'max_length': 64}. Best is trial 8 with value: 0.8562264320036017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 1.131155727098881e-05, 'num_epochs': 3, 'batch_size_train': 64, 'batch_size_eval': 64, 'warmup_steps': 668, 'weight_decay': 0.0980186282772229, 'max_length': 128}\n",
      "roberta_BATCH_SIZE_TRAIN: 64\n",
      "roberta_BATCH_SIZE_EVAL: 64\n",
      "roberta_LEARNING_RATE: 1.131155727098881e-05\n",
      "roberta_NUM_EPOCHS: 3\n",
      "roberta_WARMUP_STEPS: 668\n",
      "roberta_WEIGHT_DECAY: 0.0980186282772229\n",
      "roberta_CHOSEN_MAX_LENGTH: 128\n"
     ]
    }
   ],
   "source": [
    "def objective_roberta(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 2, 5)\n",
    "    batch_size_train = trial.suggest_categorical(\"batch_size_train\", [64, 128, 256])\n",
    "    batch_size_eval = trial.suggest_categorical(\"batch_size_eval\", [64, 128, 256])\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.001, 0.1, log=True)\n",
    "    max_length = trial.suggest_categorical(\"max_length\", [64, 128])\n",
    "\n",
    "    # ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    X_processed = preprocessor.fit_transform(X.tolist(), y.tolist())\n",
    "    X_processed = pd.Series(X_processed)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_processed, y,test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"../tapt-roberta\", num_labels=NUM_CLASSES)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_optuna\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size_train,\n",
    "        per_device_eval_batch_size=batch_size_eval,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results[\"eval_f1\"]\n",
    "\n",
    "# ìºì‹œ ì´ˆê¸°í™”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Optuna ì‹¤í–‰ (ê¸°ë³¸ ëª¨ë¸ë¡œ ìµœì í™”)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../tapt-roberta\")  # Optunaìš© í† í¬ë‚˜ì´ì €\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_roberta, n_trials=10)\n",
    "best_params_roberta = study.best_params\n",
    "print(f\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params_roberta}\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° ì ìš©\n",
    "roberta_BATCH_SIZE_TRAIN = best_params_roberta[\"batch_size_train\"]\n",
    "roberta_BATCH_SIZE_EVAL = best_params_roberta[\"batch_size_eval\"]\n",
    "roberta_LEARNING_RATE = best_params_roberta[\"learning_rate\"]\n",
    "roberta_NUM_EPOCHS = best_params_roberta[\"num_epochs\"]\n",
    "roberta_WARMUP_STEPS = best_params_roberta[\"warmup_steps\"]\n",
    "roberta_WEIGHT_DECAY = best_params_roberta[\"weight_decay\"]\n",
    "roberta_CHOSEN_MAX_LENGTH = best_params_roberta[\"max_length\"]\n",
    "print(\"roberta_BATCH_SIZE_TRAIN:\", roberta_BATCH_SIZE_TRAIN)\n",
    "print(\"roberta_BATCH_SIZE_EVAL:\", roberta_BATCH_SIZE_EVAL)\n",
    "print(\"roberta_LEARNING_RATE:\", roberta_LEARNING_RATE)\n",
    "print(\"roberta_NUM_EPOCHS:\", roberta_NUM_EPOCHS)\n",
    "print(\"roberta_WARMUP_STEPS:\", roberta_WARMUP_STEPS)\n",
    "print(\"roberta_WEIGHT_DECAY:\", roberta_WEIGHT_DECAY)\n",
    "print(\"roberta_CHOSEN_MAX_LENGTH:\", roberta_CHOSEN_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef38eb4",
   "metadata": {},
   "source": [
    "## klue/bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be29efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 23:12:47,631] A new study created in memory with name: no-name-dac957e7-d624-475e-b1ac-e85057890883\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6992' max='6992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6992/6992 21:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>0.398963</td>\n",
       "      <td>0.843608</td>\n",
       "      <td>0.839804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.379085</td>\n",
       "      <td>0.854282</td>\n",
       "      <td>0.852303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 23:36:03,274] Trial 0 finished with value: 0.8523030034731702 and parameters: {'learning_rate': 2.0210493315419464e-05, 'num_epochs': 2, 'batch_size_train': 64, 'batch_size_eval': 128, 'warmup_steps': 717, 'weight_decay': 0.06988079266802173, 'max_length': 128}. Best is trial 0 with value: 0.8523030034731702.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5244' max='5244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5244/5244 19:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.450815</td>\n",
       "      <td>0.822832</td>\n",
       "      <td>0.818615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.417973</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.832949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.418703</td>\n",
       "      <td>0.838226</td>\n",
       "      <td>0.835276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 23:56:41,924] Trial 1 finished with value: 0.8352760065640897 and parameters: {'learning_rate': 1.0065957980706629e-05, 'num_epochs': 3, 'batch_size_train': 128, 'batch_size_eval': 256, 'warmup_steps': 760, 'weight_decay': 0.005520088577921894, 'max_length': 64}. Best is trial 0 with value: 0.8523030034731702.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10488' max='10488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10488/10488 22:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.453245</td>\n",
       "      <td>0.822814</td>\n",
       "      <td>0.815154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.428796</td>\n",
       "      <td>0.836903</td>\n",
       "      <td>0.835048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.501783</td>\n",
       "      <td>0.838155</td>\n",
       "      <td>0.837472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 00:20:14,782] Trial 2 finished with value: 0.8374720132888658 and parameters: {'learning_rate': 9.10683451460436e-05, 'num_epochs': 3, 'batch_size_train': 64, 'batch_size_eval': 256, 'warmup_steps': 699, 'weight_decay': 0.0022657547524861975, 'max_length': 64}. Best is trial 0 with value: 0.8523030034731702.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6992' max='6992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6992/6992 39:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.410388</td>\n",
       "      <td>0.840837</td>\n",
       "      <td>0.836898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.378661</td>\n",
       "      <td>0.852709</td>\n",
       "      <td>0.850436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.851529</td>\n",
       "      <td>0.850073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.395208</td>\n",
       "      <td>0.853889</td>\n",
       "      <td>0.852450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 01:01:39,619] Trial 3 finished with value: 0.8524501125651771 and parameters: {'learning_rate': 1.4225178498861891e-05, 'num_epochs': 4, 'batch_size_train': 128, 'batch_size_eval': 64, 'warmup_steps': 696, 'weight_decay': 0.002246181281329394, 'max_length': 128}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5244' max='5244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5244/5244 29:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.406578</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.839595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.394898</td>\n",
       "      <td>0.851940</td>\n",
       "      <td>0.849691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.460077</td>\n",
       "      <td>0.852137</td>\n",
       "      <td>0.851928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 01:33:05,706] Trial 4 finished with value: 0.8519276357194987 and parameters: {'learning_rate': 7.858739752476041e-05, 'num_epochs': 3, 'batch_size_train': 128, 'batch_size_eval': 128, 'warmup_steps': 512, 'weight_decay': 0.003979960920569234, 'max_length': 128}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5244' max='5244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5244/5244 19:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.447901</td>\n",
       "      <td>0.824996</td>\n",
       "      <td>0.818634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.418398</td>\n",
       "      <td>0.840515</td>\n",
       "      <td>0.837930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.483872</td>\n",
       "      <td>0.840819</td>\n",
       "      <td>0.840037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 01:54:00,905] Trial 5 finished with value: 0.840037311100826 and parameters: {'learning_rate': 8.62216288884038e-05, 'num_epochs': 3, 'batch_size_train': 128, 'batch_size_eval': 128, 'warmup_steps': 744, 'weight_decay': 0.0015990465821011965, 'max_length': 64}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 24:15, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.460891</td>\n",
       "      <td>0.822188</td>\n",
       "      <td>0.816446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.409084</td>\n",
       "      <td>0.838888</td>\n",
       "      <td>0.836621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.433425</td>\n",
       "      <td>0.842160</td>\n",
       "      <td>0.839224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.474571</td>\n",
       "      <td>0.841194</td>\n",
       "      <td>0.839990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 02:19:36,782] Trial 6 finished with value: 0.8399895464271662 and parameters: {'learning_rate': 4.485423718343786e-05, 'num_epochs': 4, 'batch_size_train': 256, 'batch_size_eval': 64, 'warmup_steps': 967, 'weight_decay': 0.001331060210969572, 'max_length': 64}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5244' max='5244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5244/5244 29:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.402470</td>\n",
       "      <td>0.842643</td>\n",
       "      <td>0.839284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.378887</td>\n",
       "      <td>0.852852</td>\n",
       "      <td>0.850259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.385634</td>\n",
       "      <td>0.853299</td>\n",
       "      <td>0.851401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [437/437 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 02:51:04,626] Trial 7 finished with value: 0.8514011784158991 and parameters: {'learning_rate': 1.4136110421638435e-05, 'num_epochs': 3, 'batch_size_train': 128, 'batch_size_eval': 128, 'warmup_steps': 73, 'weight_decay': 0.0010279749229188957, 'max_length': 128}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3496' max='3496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3496/3496 13:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.441700</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.823583</td>\n",
       "      <td>0.818588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.421942</td>\n",
       "      <td>0.836403</td>\n",
       "      <td>0.832504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 03:05:42,371] Trial 8 finished with value: 0.8325044431286751 and parameters: {'learning_rate': 1.0751087821715655e-05, 'num_epochs': 2, 'batch_size_train': 128, 'batch_size_eval': 64, 'warmup_steps': 819, 'weight_decay': 0.002254967046687461, 'max_length': 64}. Best is trial 3 with value: 0.8524501125651771.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../tapt-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8740' max='8740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8740/8740 32:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.445580</td>\n",
       "      <td>0.826766</td>\n",
       "      <td>0.821450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.429480</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.834439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.489499</td>\n",
       "      <td>0.834597</td>\n",
       "      <td>0.834921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.610951</td>\n",
       "      <td>0.835133</td>\n",
       "      <td>0.833540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.744766</td>\n",
       "      <td>0.835240</td>\n",
       "      <td>0.835066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 03:39:44,412] Trial 9 finished with value: 0.8350659772882228 and parameters: {'learning_rate': 8.184238417896732e-05, 'num_epochs': 5, 'batch_size_train': 128, 'batch_size_eval': 256, 'warmup_steps': 96, 'weight_decay': 0.01807010021083869, 'max_length': 64}. Best is trial 3 with value: 0.8524501125651771.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 1.4225178498861891e-05, 'num_epochs': 4, 'batch_size_train': 128, 'batch_size_eval': 64, 'warmup_steps': 696, 'weight_decay': 0.002246181281329394, 'max_length': 128}\n",
      "bert_BATCH_SIZE_TRAIN: 128\n",
      "bert_BATCH_SIZE_EVAL: 64\n",
      "bert_LEARNING_RATE: 1.4225178498861891e-05\n",
      "bert_NUM_EPOCHS: 4\n",
      "bert_WARMUP_STEPS: 696\n",
      "bert_WEIGHT_DECAY: 0.002246181281329394\n",
      "bert_CHOSEN_MAX_LENGTH: 128\n"
     ]
    }
   ],
   "source": [
    "def objective_bert(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 2, 5)\n",
    "    batch_size_train = trial.suggest_categorical(\"batch_size_train\", [64, 128, 256])\n",
    "    batch_size_eval = trial.suggest_categorical(\"batch_size_eval\", [64, 128, 256])\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.001, 0.1, log=True)\n",
    "    max_length = trial.suggest_categorical(\"max_length\", [64, 128])\n",
    "\n",
    "    # ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    X_processed = preprocessor.fit_transform(X.tolist(), y.tolist())\n",
    "    X_processed = pd.Series(X_processed)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_processed, y,test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"../tapt-bert\", num_labels=NUM_CLASSES)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_optuna\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size_train,\n",
    "        per_device_eval_batch_size=batch_size_eval,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results[\"eval_f1\"]\n",
    "\n",
    "# ìºì‹œ ì´ˆê¸°í™”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Optuna ì‹¤í–‰ (ê¸°ë³¸ ëª¨ë¸ë¡œ ìµœì í™”)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../tapt-bert\")  # Optunaìš© í† í¬ë‚˜ì´ì €\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_bert, n_trials=10)\n",
    "best_params_bert = study.best_params\n",
    "print(f\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params_bert}\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° ì ìš©\n",
    "bert_BATCH_SIZE_TRAIN = best_params_bert[\"batch_size_train\"]\n",
    "bert_BATCH_SIZE_EVAL = best_params_bert[\"batch_size_eval\"]\n",
    "bert_LEARNING_RATE = best_params_bert[\"learning_rate\"]\n",
    "bert_NUM_EPOCHS = best_params_bert[\"num_epochs\"]\n",
    "bert_WARMUP_STEPS = best_params_bert[\"warmup_steps\"]\n",
    "bert_WEIGHT_DECAY = best_params_bert[\"weight_decay\"]\n",
    "bert_CHOSEN_MAX_LENGTH = best_params_bert[\"max_length\"]\n",
    "print(\"bert_BATCH_SIZE_TRAIN:\", bert_BATCH_SIZE_TRAIN)\n",
    "print(\"bert_BATCH_SIZE_EVAL:\", bert_BATCH_SIZE_EVAL)\n",
    "print(\"bert_LEARNING_RATE:\", bert_LEARNING_RATE)\n",
    "print(\"bert_NUM_EPOCHS:\", bert_NUM_EPOCHS)\n",
    "print(\"bert_WARMUP_STEPS:\", bert_WARMUP_STEPS)\n",
    "print(\"bert_WEIGHT_DECAY:\", bert_WEIGHT_DECAY)\n",
    "print(\"bert_CHOSEN_MAX_LENGTH:\", bert_CHOSEN_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1d3a9",
   "metadata": {},
   "source": [
    "## monologg/koelectra-base-v3-discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a76244bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 03:53:36,182] A new study created in memory with name: no-name-0eac2cce-ceb2-4091-ba53-99b7b534b223\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ../tapt-electra and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6992' max='6992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6992/6992 36:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.417179</td>\n",
       "      <td>0.843876</td>\n",
       "      <td>0.839559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.384960</td>\n",
       "      <td>0.856606</td>\n",
       "      <td>0.854580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/874 01:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 04:32:19,254] Trial 0 finished with value: 0.8545798550009707 and parameters: {'learning_rate': 9.420482412443099e-05, 'num_epochs': 2, 'batch_size_train': 64, 'batch_size_eval': 64, 'warmup_steps': 685, 'weight_decay': 0.04553144387239281, 'max_length': 256}. Best is trial 0 with value: 0.8545798550009707.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {'learning_rate': 9.420482412443099e-05, 'num_epochs': 2, 'batch_size_train': 64, 'batch_size_eval': 64, 'warmup_steps': 685, 'weight_decay': 0.04553144387239281, 'max_length': 256}\n",
      "electra_BATCH_SIZE_TRAIN: 64\n",
      "electra_BATCH_SIZE_EVAL: 64\n",
      "electra_LEARNING_RATE: 9.420482412443099e-05\n",
      "electra_NUM_EPOCHS: 2\n",
      "electra_WARMUP_STEPS: 685\n",
      "electra_WEIGHT_DECAY: 0.04553144387239281\n",
      "electra_CHOSEN_MAX_LENGTH: 256\n"
     ]
    }
   ],
   "source": [
    "def objective_electra(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 2, 5)\n",
    "    batch_size_train = trial.suggest_categorical(\"batch_size_train\", [64, 128, 256])\n",
    "    batch_size_eval = trial.suggest_categorical(\"batch_size_eval\", [64, 128, 256])\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.001, 0.1, log=True)\n",
    "    max_length = trial.suggest_categorical(\"max_length\", [64, 128, 256])\n",
    "\n",
    "    # ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬ (Optuna ë‚´ ë°˜ë³µ í”¼í•¨)\n",
    "    X_processed = preprocessor.fit_transform(X.tolist(), y.tolist())\n",
    "    X_processed = pd.Series(X_processed)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_processed, y,test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"../tapt-electra\", num_labels=NUM_CLASSES)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_optuna\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size_train,\n",
    "        per_device_eval_batch_size=batch_size_eval,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results[\"eval_f1\"]\n",
    "\n",
    "# # ìºì‹œ ì´ˆê¸°í™”\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# Optuna ì‹¤í–‰ (ê¸°ë³¸ ëª¨ë¸ë¡œ ìµœì í™”)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../tapt-electra\")  # Optunaìš© í† í¬ë‚˜ì´ì €\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_electra, n_trials=1)\n",
    "best_params_electra = study.best_params\n",
    "print(f\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params_electra}\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„° ì ìš©\n",
    "electra_BATCH_SIZE_TRAIN = best_params_electra[\"batch_size_train\"]\n",
    "electra_BATCH_SIZE_EVAL = best_params_electra[\"batch_size_eval\"]\n",
    "electra_LEARNING_RATE = best_params_electra[\"learning_rate\"]\n",
    "electra_NUM_EPOCHS = best_params_electra[\"num_epochs\"]\n",
    "electra_WARMUP_STEPS = best_params_electra[\"warmup_steps\"]\n",
    "electra_WEIGHT_DECAY = best_params_electra[\"weight_decay\"]\n",
    "electra_CHOSEN_MAX_LENGTH = best_params_electra[\"max_length\"]\n",
    "print(\"electra_BATCH_SIZE_TRAIN:\", electra_BATCH_SIZE_TRAIN)\n",
    "print(\"electra_BATCH_SIZE_EVAL:\", electra_BATCH_SIZE_EVAL)\n",
    "print(\"electra_LEARNING_RATE:\", electra_LEARNING_RATE)\n",
    "print(\"electra_NUM_EPOCHS:\", electra_NUM_EPOCHS)\n",
    "print(\"electra_WARMUP_STEPS:\", electra_WARMUP_STEPS)\n",
    "print(\"electra_WEIGHT_DECAY:\", electra_WEIGHT_DECAY)\n",
    "print(\"electra_CHOSEN_MAX_LENGTH:\", electra_CHOSEN_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a08195",
   "metadata": {},
   "source": [
    "# ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f8fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ì™„ë£Œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
